{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-02T03:23:43.486148Z",
     "start_time": "2025-07-02T03:23:42.621003Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib_inline import backend_inline\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "operator torchvision::nms does not exist",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmatplotlib_inline\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m backend_inline\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01md2l\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m torch \u001B[38;5;28;01mas\u001B[39;00m d2l\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mf\u001B[39m(x):\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m3\u001B[39m \u001B[38;5;241m*\u001B[39m x \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m4\u001B[39m \u001B[38;5;241m*\u001B[39m x\n",
      "File \u001B[0;32m/opt/miniconda3/envs/PyCharmMiscProject/lib/python3.10/site-packages/d2l/torch.py:6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorchvision\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mPIL\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m nn\n",
      "File \u001B[0;32m/opt/miniconda3/envs/PyCharmMiscProject/lib/python3.10/site-packages/torchvision/__init__.py:10\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# .extensions) before entering _meta_registrations.\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mextension\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _HAS_OPS  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m __version__  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/PyCharmMiscProject/lib/python3.10/site-packages/torchvision/_meta_registrations.py:164\u001B[0m\n\u001B[1;32m    153\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_check(\n\u001B[1;32m    154\u001B[0m         grad\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m rois\u001B[38;5;241m.\u001B[39mdtype,\n\u001B[1;32m    155\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m: (\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    158\u001B[0m         ),\n\u001B[1;32m    159\u001B[0m     )\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m grad\u001B[38;5;241m.\u001B[39mnew_empty((batch_size, channels, height, width))\n\u001B[1;32m    163\u001B[0m \u001B[38;5;129;43m@torch\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlibrary\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mregister_fake\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtorchvision::nms\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m--> 164\u001B[0m \u001B[38;5;28;43;01mdef\u001B[39;49;00m\u001B[38;5;250;43m \u001B[39;49m\u001B[38;5;21;43mmeta_nms\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscores\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miou_threshold\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdim\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mboxes should be a 2d tensor, got \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdim\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43mD\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mboxes should have 4 elements in dimension 1, got \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/PyCharmMiscProject/lib/python3.10/site-packages/torch/library.py:1023\u001B[0m, in \u001B[0;36mregister\u001B[0;34m(func)\u001B[0m\n\u001B[1;32m   1012\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mregister_vmap\u001B[39m(\n\u001B[1;32m   1013\u001B[0m     op: _op_identifier,\n\u001B[1;32m   1014\u001B[0m     func: Optional[Callable] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1017\u001B[0m     lib\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1018\u001B[0m ):\n\u001B[1;32m   1019\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Register a vmap implementation to support :func:`torch.vmap` for this custom op.\u001B[39;00m\n\u001B[1;32m   1020\u001B[0m \n\u001B[1;32m   1021\u001B[0m \u001B[38;5;124;03m    This API may be used as a decorator (see examples).\u001B[39;00m\n\u001B[1;32m   1022\u001B[0m \n\u001B[0;32m-> 1023\u001B[0m \u001B[38;5;124;03m    In order for an operator to work with :func:`torch.vmap`, you may need to register a\u001B[39;00m\n\u001B[1;32m   1024\u001B[0m \u001B[38;5;124;03m    vmap implementation in the following signature:\u001B[39;00m\n\u001B[1;32m   1025\u001B[0m \n\u001B[1;32m   1026\u001B[0m \u001B[38;5;124;03m        ``vmap_func(info, in_dims: Tuple[Optional[int]], *args, **kwargs)``,\u001B[39;00m\n\u001B[1;32m   1027\u001B[0m \n\u001B[1;32m   1028\u001B[0m \u001B[38;5;124;03m    where ``*args`` and ``**kwargs`` are the arguments and kwargs for ``op``.\u001B[39;00m\n\u001B[1;32m   1029\u001B[0m \u001B[38;5;124;03m    We do not support kwarg-only Tensor args.\u001B[39;00m\n\u001B[1;32m   1030\u001B[0m \n\u001B[1;32m   1031\u001B[0m \u001B[38;5;124;03m    It specifies how do we compute the batched version of ``op`` given inputs with an additional\u001B[39;00m\n\u001B[1;32m   1032\u001B[0m \u001B[38;5;124;03m    dimension (specified by ``in_dims``).\u001B[39;00m\n\u001B[1;32m   1033\u001B[0m \n\u001B[1;32m   1034\u001B[0m \u001B[38;5;124;03m    For each arg in ``args``, ``in_dims`` has a corresponding ``Optional[int]``. It is ``None``\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;124;03m    if the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer\u001B[39;00m\n\u001B[1;32m   1036\u001B[0m \u001B[38;5;124;03m    specifying what dimension of the Tensor is being vmapped over.\u001B[39;00m\n\u001B[1;32m   1037\u001B[0m \n\u001B[1;32m   1038\u001B[0m \u001B[38;5;124;03m    ``info`` is a collection of additional metadata that may be helpful:\u001B[39;00m\n\u001B[1;32m   1039\u001B[0m \u001B[38;5;124;03m    ``info.batch_size`` specifies the size of the dimension being vmapped over, while\u001B[39;00m\n\u001B[1;32m   1040\u001B[0m \u001B[38;5;124;03m    ``info.randomness`` is the ``randomness`` option that was passed to :func:`torch.vmap`.\u001B[39;00m\n\u001B[1;32m   1041\u001B[0m \n\u001B[1;32m   1042\u001B[0m \u001B[38;5;124;03m    The return of the function ``func`` is a tuple of ``(output, out_dims)``. Similar to ``in_dims``,\u001B[39;00m\n\u001B[1;32m   1043\u001B[0m \u001B[38;5;124;03m    ``out_dims`` should be of the same structure as ``output`` and contain one ``out_dim``\u001B[39;00m\n\u001B[1;32m   1044\u001B[0m \u001B[38;5;124;03m    per output that specifies if the output has the vmapped dimension and what index it is in.\u001B[39;00m\n\u001B[1;32m   1045\u001B[0m \n\u001B[1;32m   1046\u001B[0m \u001B[38;5;124;03m    Examples:\u001B[39;00m\n\u001B[1;32m   1047\u001B[0m \u001B[38;5;124;03m        >>> import torch\u001B[39;00m\n\u001B[1;32m   1048\u001B[0m \u001B[38;5;124;03m        >>> import numpy as np\u001B[39;00m\n\u001B[1;32m   1049\u001B[0m \u001B[38;5;124;03m        >>> from torch import Tensor\u001B[39;00m\n\u001B[1;32m   1050\u001B[0m \u001B[38;5;124;03m        >>> from typing import Tuple\u001B[39;00m\n\u001B[1;32m   1051\u001B[0m \u001B[38;5;124;03m        >>>\u001B[39;00m\n\u001B[1;32m   1052\u001B[0m \u001B[38;5;124;03m        >>> def to_numpy(tensor):\u001B[39;00m\n\u001B[1;32m   1053\u001B[0m \u001B[38;5;124;03m        >>>     return tensor.cpu().numpy()\u001B[39;00m\n\u001B[1;32m   1054\u001B[0m \u001B[38;5;124;03m        >>>\u001B[39;00m\n\u001B[1;32m   1055\u001B[0m \u001B[38;5;124;03m        >>> lib = torch.library.Library(\"mylib\", \"FRAGMENT\")\u001B[39;00m\n\u001B[1;32m   1056\u001B[0m \u001B[38;5;124;03m        >>> @torch.library.custom_op(\"mylib::numpy_cube\", mutates_args=())\u001B[39;00m\n\u001B[1;32m   1057\u001B[0m \u001B[38;5;124;03m        >>> def numpy_cube(x: Tensor) -> Tuple[Tensor, Tensor]:\u001B[39;00m\n\u001B[1;32m   1058\u001B[0m \u001B[38;5;124;03m        >>>     x_np = to_numpy(x)\u001B[39;00m\n\u001B[1;32m   1059\u001B[0m \u001B[38;5;124;03m        >>>     dx = torch.tensor(3 * x_np ** 2, device=x.device)\u001B[39;00m\n\u001B[1;32m   1060\u001B[0m \u001B[38;5;124;03m        >>>     return torch.tensor(x_np ** 3, device=x.device), dx\u001B[39;00m\n\u001B[1;32m   1061\u001B[0m \u001B[38;5;124;03m        >>>\u001B[39;00m\n\u001B[1;32m   1062\u001B[0m \u001B[38;5;124;03m        >>> def numpy_cube_vmap(info, in_dims, x):\u001B[39;00m\n\u001B[1;32m   1063\u001B[0m \u001B[38;5;124;03m        >>>     result = numpy_cube(x)\u001B[39;00m\n\u001B[1;32m   1064\u001B[0m \u001B[38;5;124;03m        >>>     return result, (in_dims[0], in_dims[0])\u001B[39;00m\n\u001B[1;32m   1065\u001B[0m \u001B[38;5;124;03m        >>>\u001B[39;00m\n\u001B[1;32m   1066\u001B[0m \u001B[38;5;124;03m        >>> torch.library.register_vmap(numpy_cube, numpy_cube_vmap)\u001B[39;00m\n\u001B[1;32m   1067\u001B[0m \u001B[38;5;124;03m        >>>\u001B[39;00m\n\u001B[1;32m   1068\u001B[0m \u001B[38;5;124;03m        >>> x = torch.randn(3)\u001B[39;00m\n\u001B[1;32m   1069\u001B[0m \u001B[38;5;124;03m        >>> torch.vmap(numpy_cube)(x)\u001B[39;00m\n\u001B[1;32m   1070\u001B[0m \u001B[38;5;124;03m        >>>\u001B[39;00m\n\u001B[1;32m   1071\u001B[0m \u001B[38;5;124;03m        >>> @torch.library.custom_op(\"mylib::numpy_mul\", mutates_args=())\u001B[39;00m\n\u001B[1;32m   1072\u001B[0m \u001B[38;5;124;03m        >>> def numpy_mul(x: Tensor, y: Tensor) -> Tensor:\u001B[39;00m\n\u001B[1;32m   1073\u001B[0m \u001B[38;5;124;03m        >>>     return torch.tensor(to_numpy(x) * to_numpy(y), device=x.device)\u001B[39;00m\n\u001B[1;32m   1074\u001B[0m \u001B[38;5;124;03m        >>>\u001B[39;00m\n\u001B[1;32m   1075\u001B[0m \u001B[38;5;124;03m        >>> @torch.library.register_vmap(\"mylib::numpy_mul\")\u001B[39;00m\n\u001B[1;32m   1076\u001B[0m \u001B[38;5;124;03m        >>> def numpy_mul_vmap(info, in_dims, x, y):\u001B[39;00m\n\u001B[1;32m   1077\u001B[0m \u001B[38;5;124;03m        >>>     x_bdim, y_bdim = in_dims\u001B[39;00m\n\u001B[1;32m   1078\u001B[0m \u001B[38;5;124;03m        >>>     x = x.movedim(x_bdim, -1) if x_bdim is not None else x.unsqueeze(-1)\u001B[39;00m\n\u001B[1;32m   1079\u001B[0m \u001B[38;5;124;03m        >>>     y = y.movedim(y_bdim, -1) if y_bdim is not None else y.unsqueeze(-1)\u001B[39;00m\n\u001B[1;32m   1080\u001B[0m \u001B[38;5;124;03m        >>>     result = x * y\u001B[39;00m\n\u001B[1;32m   1081\u001B[0m \u001B[38;5;124;03m        >>>     result = result.movedim(-1, 0)\u001B[39;00m\n\u001B[1;32m   1082\u001B[0m \u001B[38;5;124;03m        >>>     return result, 0\u001B[39;00m\n\u001B[1;32m   1083\u001B[0m \u001B[38;5;124;03m        >>>\u001B[39;00m\n\u001B[1;32m   1084\u001B[0m \u001B[38;5;124;03m        >>>\u001B[39;00m\n\u001B[1;32m   1085\u001B[0m \u001B[38;5;124;03m        >>> x = torch.randn(3)\u001B[39;00m\n\u001B[1;32m   1086\u001B[0m \u001B[38;5;124;03m        >>> y = torch.randn(3)\u001B[39;00m\n\u001B[1;32m   1087\u001B[0m \u001B[38;5;124;03m        >>> torch.vmap(numpy_mul)(x, y)\u001B[39;00m\n\u001B[1;32m   1088\u001B[0m \n\u001B[1;32m   1089\u001B[0m \u001B[38;5;124;03m    .. note::\u001B[39;00m\n\u001B[1;32m   1090\u001B[0m \u001B[38;5;124;03m        The vmap function should aim to preserve the semantics of the entire custom operator.\u001B[39;00m\n\u001B[1;32m   1091\u001B[0m \u001B[38;5;124;03m        That is, ``grad(vmap(op))`` should be replaceable with a ``grad(map(op))``.\u001B[39;00m\n\u001B[1;32m   1092\u001B[0m \n\u001B[1;32m   1093\u001B[0m \u001B[38;5;124;03m        If your custom operator has any custom behavior in the backward pass, please\u001B[39;00m\n\u001B[1;32m   1094\u001B[0m \u001B[38;5;124;03m        keep this in mind.\u001B[39;00m\n\u001B[1;32m   1095\u001B[0m \n\u001B[1;32m   1096\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   1097\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m   1098\u001B[0m         op, (\u001B[38;5;28mstr\u001B[39m, torch\u001B[38;5;241m.\u001B[39m_ops\u001B[38;5;241m.\u001B[39mOpOverload, torch\u001B[38;5;241m.\u001B[39m_library\u001B[38;5;241m.\u001B[39mcustom_ops\u001B[38;5;241m.\u001B[39mCustomOpDef)\n\u001B[1;32m   1099\u001B[0m     ):\n\u001B[1;32m   1100\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mregister_vmap(op): got unexpected type for op: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(op)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/PyCharmMiscProject/lib/python3.10/site-packages/torch/library.py:214\u001B[0m, in \u001B[0;36m_register_fake\u001B[0;34m(self, op_name, fn, _stacklevel)\u001B[0m\n\u001B[1;32m    208\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_impl_with_aoti_compile\u001B[39m(\u001B[38;5;28mself\u001B[39m, op_name, dispatch_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    209\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Register the operator to use the AOTI-compiled implementation.\u001B[39;00m\n\u001B[1;32m    210\u001B[0m \n\u001B[1;32m    211\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;124;03m        op_name: operator name (along with the overload) or OpOverload object.\u001B[39;00m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;124;03m        dispatch_key: dispatch key that the input function should be registered for. By default, it uses\u001B[39;00m\n\u001B[0;32m--> 214\u001B[0m \u001B[38;5;124;03m                      the dispatch key that the library was created with.\u001B[39;00m\n\u001B[1;32m    215\u001B[0m \n\u001B[1;32m    216\u001B[0m \u001B[38;5;124;03m    Example::\u001B[39;00m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;124;03m        >>> my_lib = Library(\"aten\", \"IMPL\")\u001B[39;00m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;124;03m        >>> my_lib._impl_with_aoti_compile(\"div.Tensor\", \"CPU\")\u001B[39;00m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    220\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dispatch_key \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    221\u001B[0m         dispatch_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_key\n",
      "File \u001B[0;32m/opt/miniconda3/envs/PyCharmMiscProject/lib/python3.10/site-packages/torch/_library/fake_impl.py:31\u001B[0m, in \u001B[0;36mFakeImplHolder.register\u001B[0;34m(self, func, source)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m     27\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mregister_fake(...): the operator \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqualname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     28\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124malready has an fake impl registered at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     29\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel\u001B[38;5;241m.\u001B[39msource\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     30\u001B[0m     )\n\u001B[0;32m---> 31\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch_has_kernel_for_dispatch_key\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mqualname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mMeta\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m     33\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mregister_fake(...): the operator \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqualname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     34\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124malready has an DispatchKey::Meta implementation via a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     37\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mregister_fake.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     38\u001B[0m     )\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_dispatch_has_kernel_for_dispatch_key(\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqualname, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCompositeImplicitAutograd\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     42\u001B[0m ):\n",
      "\u001B[0;31mRuntimeError\u001B[0m: operator torchvision::nms does not exist"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T03:25:10.820631Z",
     "start_time": "2025-07-02T03:25:10.773384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ],
   "id": "b037f4fb30b8dd38",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorchvision\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(torch\u001B[38;5;241m.\u001B[39m__version__)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(torchvision\u001B[38;5;241m.\u001B[39m__version__)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/PyCharmMiscProject/lib/python3.10/site-packages/torchvision/__init__.py:10\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# .extensions) before entering _meta_registrations.\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mextension\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _HAS_OPS  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m __version__  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/PyCharmMiscProject/lib/python3.10/site-packages/torchvision/_meta_registrations.py:26\u001B[0m\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m fn\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m wrapper\n\u001B[1;32m     25\u001B[0m \u001B[38;5;129;43m@register_meta\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mroi_align\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m---> 26\u001B[0m \u001B[38;5;28;43;01mdef\u001B[39;49;00m\u001B[38;5;250;43m \u001B[39;49m\u001B[38;5;21;43mmeta_roi_align\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrois\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspatial_scale\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpooled_height\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpooled_width\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampling_ratio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maligned\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrois\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrois must have shape as Tensor[K, 5]\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mrois\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/PyCharmMiscProject/lib/python3.10/site-packages/torchvision/_meta_registrations.py:18\u001B[0m, in \u001B[0;36mregister_meta.<locals>.wrapper\u001B[0;34m(fn)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mwrapper\u001B[39m(fn):\n\u001B[0;32m---> 18\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtorchvision\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextension\u001B[49m\u001B[38;5;241m.\u001B[39m_has_ops():\n\u001B[1;32m     19\u001B[0m         get_meta_lib()\u001B[38;5;241m.\u001B[39mimpl(\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mgetattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mtorchvision, op_name), overload_name), fn)\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn\n",
      "\u001B[0;31mAttributeError\u001B[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
